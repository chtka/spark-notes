{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Spark Programming Model\n",
    "- Spark programming consists of operations on a data set, usually residing in some form of distributed, persistent storage (e.g. HDFS)\n",
    "- consists of the following steps:\n",
    "    - Define a set of transformations on the input data set.\n",
    "    - Invoke actions that output the transformed data sets to persistent storage or return results to the driver's local memory.\n",
    "    - Run local computations that operate on the results computed in a distributed fashion.\n",
    "    \n",
    "### Record Linkage\n",
    "- the problem of tying multiple duplicate records to the same underlying entity when we have a large collection of records from one or more source systems\n",
    "- difficulty comes from the fact that criteria for determining duplicate/not-duplicate varies from a case to case basis\n",
    "    - in some cases, very different looking records will refer to the same entity, and in other case, very similar looking records will actually refer to different entities despite the similarity\n",
    "    \n",
    "##### spark-shell instructions\n",
    "- if running examples on personal computer, can launch a local Spark cluster by specifying ```--master local[N]```, where N is the number of threads to run\n",
    "    - specifying local[\\*] will match the number of threads to the number of cores available on machine\n",
    "- other arguments\n",
    "    - ```--driver-memory 2g``` -> lets single local process use 2 GB of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.SparkContext@7dda3abe"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// The SparkContext object\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resilient Distributed Datasets\n",
    "- ```SparkContext``` has methods that allow us to create _Resilient Distributed Datasets_, or _RDDs_, which are Spark's abstraction for representing a collection of objects that can be distributed across multiple machines in a cluster\n",
    "- two ways to create _RDDs_\n",
    "    - use ```SparkContext``` to create RDD from external data source\n",
    "    - perform a transformation on one or more existing RDDs, yielding an RDD as a result (e.g. filtering records, aggregating records by common key, joining multiple RDDs together)\n",
    "- _RDDs_ are laid out across the cluster of machines as a collection of _partitions_, each including a subset of the data\n",
    "    - Spark then processes the objects within a partition in sequence, and processes multiple partitions in parallel\n",
    "- One simple way to create an RDD is to use ```parallelize``` method on  ```SparkContext``` with a local collection of objects\n",
    "    - first arg is the collection of objects to parallelize, in an ```Array```\n",
    "    - second arg is number of partitions to create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "val rdd = sc.parallelize(Array(1, 2, 2, 4), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- to create RDD from text file or directory of text files, pass the name of the file or directory to ```textFile``` \n",
    "    - ```textFile``` can access paths that reside on the local file system\n",
    "    - if given a directory, it will consider all of the files in that directory as part of the given RDD\n",
    "    - no data has yet been read by Spark or loaded into memory yet; instead, objects are loaded into the cluster at computation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "val rawblocks = sc.textFile(\"linkage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The REPL and Compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spark supports both interactive shell and compiled applications, which can be compiled and managed using _Apache Maven_\n",
    "- shell method\n",
    "    - starting work in the REPL enables quick prototyping, faster iteration, and less lag between ideas and results\n",
    "    - drawbacks: not suited for large programs, since Scala interpretation takes longer\n",
    "- hybrid method\n",
    "    - develop in the REPL, but move established pieces of code into compiled library\n",
    "    - ```spark-shell``` can use compiled JAR files with the ```--jars``` flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing Data from the Cluster to the Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RDDs have various method allowing to read data from cluster into Scala REPL\n",
    "- ```RDD.first``` returns the first element of the RDD into the client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"id_1\",\"id_2\",\"cmp_fname_c1\",\"cmp_fname_c2\",\"cmp_lname_c1\",\"cmp_lname_c2\",\"cmp_sex\",\"cmp_bd\",\"cmp_bm\",\"cmp_by\",\"cmp_plz\",\"is_match\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawblocks.first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```RDD.collect``` returns all the contents of an RDD to the client as an array\n",
    "    - not recommended for huge data sets\n",
    "- ```RDD.take``` allows us to read a given nmber of records into an array on the client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val head = rawblocks.take(10)\n",
    "head.length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- creating an RDD does not cause distrbuted computation to take place on the cluster\n",
    "- instead, RDDs define logical data sets that are more like intermediate computation steps\n",
    "- distributed computation occurs upon invoking an _action_ on an RDD\n",
    "    - e.g. ```count``` action return # objects in RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1, 2, 2, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// brings objects from the RDD into local memory as an Array\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```saveAsTextFile``` saves contents of RDD to persistent storage\n",
    "    - creates a directory, and writes out each partition as a separate file\n",
    "    - this created directory might be used as an input directory by a future Spark job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.hadoop.mapred.FileAlreadyExistsException\n",
       "Message: Output directory file:/home/chtka/Projects/Spark/numbers already exists\n",
       "StackTrace:   at org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n",
       "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1184)\n",
       "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1161)\n",
       "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1161)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n",
       "  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1161)\n",
       "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1064)\n",
       "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1030)\n",
       "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1030)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n",
       "  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1030)\n",
       "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:956)\n",
       "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:956)\n",
       "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:956)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n",
       "  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:955)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1459)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1438)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1438)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n",
       "  at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1438)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.saveAsTextFile(\"numbers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1, 2, 4, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var rdd2 = sc.textFile(\"numbers\")\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```foreach``` method can be used in conjunction with ```println``` to print out each value in the array on its own line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"id_1\",\"id_2\",\"cmp_fname_c1\",\"cmp_fname_c2\",\"cmp_lname_c1\",\"cmp_lname_c2\",\"cmp_sex\",\"cmp_bd\",\"cmp_bm\",\"cmp_by\",\"cmp_plz\",\"is_match\"\n",
      "6698,40542,1,1,1,?,1,1,1,1,1,TRUE\n",
      "45037,49220,1,?,1,?,1,1,1,1,1,TRUE\n",
      "31835,69902,1,?,1,1,1,1,1,1,1,TRUE\n",
      "4356,31352,0.875,?,1,?,1,1,1,1,1,TRUE\n",
      "45723,49837,1,?,1,?,1,1,1,1,1,TRUE\n",
      "39716,49297,1,?,1,?,1,1,1,1,1,TRUE\n",
      "71970,71971,1,?,1,?,1,1,1,1,1,TRUE\n",
      "96601,96625,1,?,1,?,1,1,1,1,1,TRUE\n",
      "28553,71491,1,?,1,?,1,1,1,1,1,TRUE\n"
     ]
    }
   ],
   "source": [
    "head.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- examining the data, we see a header row that we might want to remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isHeader(line: String) = line.contains(\"id_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(6698,40542,1,1,1,?,1,1,1,1,1,TRUE, 45037,49220,1,?,1,?,1,1,1,1,1,TRUE, 31835,69902,1,?,1,1,1,1,1,1,1,TRUE, 4356,31352,0.875,?,1,?,1,1,1,1,1,TRUE, 45723,49837,1,?,1,?,1,1,1,1,1,TRUE, 39716,49297,1,?,1,?,1,1,1,1,1,TRUE, 71970,71971,1,?,1,?,1,1,1,1,1,TRUE, 96601,96625,1,?,1,?,1,1,1,1,1,TRUE, 28553,71491,1,?,1,?,1,1,1,1,1,TRUE)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// head.filterNot(isHeader)\n",
    "head.filter(x => !isHeader(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shipping Code from the Client to the Cluster\n",
    "- we can interactively develop and debug data-munging code against a small amount of data that we sample from the cluster before applying to the entire data set when we're ready to transform it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val noheader = rawblocks.filter(x => !isHeader(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6698,40542,1,1,1,?,1,1,1,1,1,TRUE"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noheader.first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From RDDs to Data Frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spark's ```DataFrame``` is an abstraction built on top of RDDs for data sets with regular structure\n",
    "    - each row is made up of a set of columns, and each column has well-defined data type\n",
    "    - basically Spark analogue of a table in a relational databse\n",
    "    - differ from Python's ```pandas.DataFrame``` in that they represent distributed data sets on a cluster, instead of local data\n",
    "- ```SparkSession``` is a wrapper around the ```SparkContext``` object\n",
    "- can create a Data Frame from ```csv``` method on ```SparkSession```'s Reader API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "val prev = spark.read.csv(\"linkage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----+----+----+----+----+----+----+----+----+\n",
      "|                 _c0|                 _c1|                 _c2| _c3| _c4| _c5| _c6| _c7| _c8| _c9|_c10|_c11|\n",
      "+--------------------+--------------------+--------------------+----+----+----+----+----+----+----+----+----+\n",
      "|                PK\u0003\u0004|                null|                null|null|null|null|null|null|null|null|null|null|\n",
      "|�5�\u0018_�}�%˄�C3���\u0011...|                null|                null|null|null|null|null|null|null|null|null|null|\n",
      "|�\b����IHr\u0002���`oP9...|                null|                null|null|null|null|null|null|null|null|null|null|\n",
      "|O�g������Z%����!L...|&9��>�tjm�U)�=��\u001c",
      "...|�.��s�\u0016겒�;��P$^-^...|null|null|null|null|null|null|null|null|null|\n",
      "|     \u0012ϊa\\?���b��6�>�|                null|                null|null|null|null|null|null|null|null|null|null|\n",
      "|^k\u001d",
      "�ç}�۔HKlP2{��!...|                null|                null|null|null|null|null|null|null|null|null|null|\n",
      "|              ��2�È�|v�R�.\u0004\u001ay�툲�\u0013�$��$...|                null|null|null|null|null|null|null|null|null|null|\n",
      "|Zn��%�&�܇�J\t\f",
      "�J��...|                null|                null|null|null|null|null|null|null|null|null|null|\n",
      "|$\u001b���N�z%��O'\u0015�jE...|                null|                null|null|null|null|null|null|null|null|null|null|\n",
      "||d'��W;�}���1x�f...|                null|                null|null|null|null|null|null|null|null|null|null|\n",
      "|lu7�ВR�ضڽ_lU��g��...|                null|                null|null|null|null|null|null|null|null|null|null|\n",
      "| ��i\u001e",
      "�a\u0010w�X�\b�y\f",
      ";�\u0014�|\u0006�F;��;|��\u0017Nv=\u0018\u0013\u0007...|                null|null|null|null|null|null|null|null|null|null|\n",
      "|           \u001c",
      "�^�mݛ�6�|                null|                null|null|null|null|null|null|null|null|null|null|\n",
      "|j\u0005�d���2ع��%:5��8...|Ouț\u0017��i�b�'�r�\t8�...|                null|null|null|null|null|null|null|null|null|null|\n",
      "|�P�[��bQnB�C>�\u0003�r...|                null|                null|null|null|null|null|null|null|null|null|null|\n",
      "|6`RތX�>��ğ�Tƣ{\u00196\u0004...|                null|                null|null|null|null|null|null|null|null|null|null|\n",
      "|��8�N����ӳ���\u0001r �...|                null|                null|null|null|null|null|null|null|null|null|null|\n",
      "|����[��^��h�u\u0014\u0014Xj...|��.�\u000fž�6\b��t��@�&...|                null|null|null|null|null|null|null|null|null|null|\n",
      "|N�Jv��\f",
      "�ه\u001b0���h��...|                null|                null|null|null|null|null|null|null|null|null|null|\n",
      "|�r�@b�zn\u0003�/��\t��6...|                null|                null|null|null|null|null|null|null|null|null|null|\n",
      "+--------------------+--------------------+--------------------+----+----+----+----+----+----+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prev.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spark can do some data processing while parsing, like inferring column names from a header, recognizing null values, and inferring the data types of each column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "val parsed = spark.read.\n",
    "    option(\"header\", \"true\").\n",
    "    option(\"nullValue\", \"?\").\n",
    "    option(\"inferSchema\", \"true\").\n",
    "    csv(\"linkage1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "| id_1| id_2|cmp_fname_c1|cmp_fname_c2|cmp_lname_c1|cmp_lname_c2|cmp_sex|cmp_bd|cmp_bm|cmp_by|cmp_plz|is_match|\n",
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "| 3148| 8326|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|14055|94934|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|33948|34740|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|  946|71870|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|64880|71676|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|25739|45991|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|62415|93584|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      0|    true|\n",
      "|27995|31399|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "| 4909|12238|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|15161|16743|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|31703|37310|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|30213|36558|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|56596|56630|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|16481|21174|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|32649|37094|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|34268|37260|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|66117|69253|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      0|    true|\n",
      "| 2771|31982|         1.0|        null|         1.0|        null|      0|     1|     1|     1|      1|    true|\n",
      "|23557|29673|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|37156|39557|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we can examine the schema of the ```parsed``` Data Frame with ```printSchema```\n",
    "    - each ```StructField``` contains the name of the column, the most specific data type that could handle the type of data contained in each record, a a boolean field that indicates whether a column may contain null values\n",
    "    - to do this, Spark does _two_ passes over the data set: one pass to figure out column types, and a second pass to do the actual parsing\n",
    "    - if schema is known in advance, can create instance of ```org.apache.spark.sql.types.StructType``` and pass to Reader API via ```schema``` function, possibly saving significant resources when the data set is very large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_1: integer (nullable = true)\n",
      " |-- id_2: integer (nullable = true)\n",
      " |-- cmp_fname_c1: double (nullable = true)\n",
      " |-- cmp_fname_c2: double (nullable = true)\n",
      " |-- cmp_lname_c1: double (nullable = true)\n",
      " |-- cmp_lname_c2: double (nullable = true)\n",
      " |-- cmp_sex: integer (nullable = true)\n",
      " |-- cmp_bd: integer (nullable = true)\n",
      " |-- cmp_bm: integer (nullable = true)\n",
      " |-- cmp_by: integer (nullable = true)\n",
      " |-- cmp_plz: integer (nullable = true)\n",
      " |-- is_match: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- through ```DataFrameReader``` and ```DataFrameWriter``` APIs, Spark supports reading and writing data frames in a variety of formats\n",
    "    - _json_ - similar functionality to CSV format\n",
    "    - _parquet_ and _orc_ - columnar-oriented binary file formats\n",
    "    - _jdbc_ - connects to relational database via JDBC data connection standard\n",
    "    - _libsvm_ - popular text file format for representing labeled observations with sparse features\n",
    "    - _text_ - maps each line of a file to a data frame with a single column of type ```String```\n",
    "- access ```DataFrameReader``` API through ```read``` method on a ```SparkSession``` instance\n",
    "    - load data from file using either combination of ```format``` and ```load``` methods, or one of the shortcuts for built-in formats\n",
    "- to write out data, access ```DataFrameWriter``` via ```write``` method on any DataFrame Instance\n",
    "- Spark will throw error if you try to save data frame to file that already exists by default; control this behavior using ```SaveMode``` enum, with ```Overwrite```, ```Append```, and ```Ignore``` options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// val d1 = spark.read.format(\"json\").load(\"file.json\")\n",
    "// val d2 = spark.read.json(\"file.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Data with the DataFrame API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Every time we've processed the data set, Spark has re-opened the file, reparsed the rows, and then perform the action requested\n",
    "- Instead of doing this, we can save the data in its parsed form on teh cluster\n",
    "- we can accomplish via the ```cache``` method on the Data Frame instance\n",
    "- ```cache``` call indicates that contents of DataFrame should be stored in memory the next time it's computed\n",
    "    - so in this example, the call to ```count``` does the re-opening, reparsing, and action (counting)\n",
    "    - the call to ```take``` accesses the cached data instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[id_1: int, id_2: int ... 10 more fields]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5749132"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([3148,8326,1.0,null,1.0,null,1,1,1,1,1,true], [14055,94934,1.0,null,1.0,null,1,1,1,1,1,true], [33948,34740,1.0,null,1.0,null,1,1,1,1,1,true], [946,71870,1.0,null,1.0,null,1,1,1,1,1,true], [64880,71676,1.0,null,1.0,null,1,1,1,1,1,true], [25739,45991,1.0,null,1.0,null,1,1,1,1,1,true], [62415,93584,1.0,null,1.0,null,1,1,1,1,0,true], [27995,31399,1.0,null,1.0,null,1,1,1,1,1,true], [4909,12238,1.0,null,1.0,null,1,1,1,1,1,true], [15161,16743,1.0,null,1.0,null,1,1,1,1,1,true])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
