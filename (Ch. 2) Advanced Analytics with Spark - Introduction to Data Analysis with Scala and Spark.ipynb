{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Spark Programming Model\n",
    "- Spark programming consists of operations on a data set, usually residing in some form of distributed, persistent storage (e.g. HDFS)\n",
    "- consists of the following steps:\n",
    "    - Define a set of transformations on the input data set.\n",
    "    - Invoke actions that output the transformed data sets to persistent storage or return results to the driver's local memory.\n",
    "    - Run local computations that operate on the results computed in a distributed fashion.\n",
    "    \n",
    "### Record Linkage\n",
    "- the problem of tying multiple duplicate records to the same underlying entity when we have a large collection of records from one or more source systems\n",
    "- difficulty comes from the fact that criteria for determining duplicate/not-duplicate varies from a case to case basis\n",
    "    - in some cases, very different looking records will refer to the same entity, and in other case, very similar looking records will actually refer to different entities despite the similarity\n",
    "    \n",
    "##### spark-shell instructions\n",
    "- if running examples on personal computer, can launch a local Spark cluster by specifying ```--master local[N]```, where N is the number of threads to run\n",
    "    - specifying local[\\*] will match the number of threads to the number of cores available on machine\n",
    "- other arguments\n",
    "    - ```--driver-memory 2g``` -> lets single local process use 2 GB of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ul>\n",
       "<li><a href=\"Some(http://100.83.15.197:4040)\" target=\"new_tab\">Spark UI: local-1521294165787</a></li>\n",
       "</ul>"
      ],
      "text/plain": [
       "Spark local-1521294165787: Some(http://100.83.15.197:4040)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// The SparkContext object\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resilient Distributed Datasets\n",
    "- ```SparkContext``` has methods that allow us to create _Resilient Distributed Datasets_, or _RDDs_, which are Spark's abstraction for representing a collection of objects that can be distributed across multiple machines in a cluster\n",
    "- two ways to create _RDDs_\n",
    "    - use ```SparkContext``` to create RDD from external data source\n",
    "    - perform a transformation on one or more existing RDDs, yielding an RDD as a result (e.g. filtering records, aggregating records by common key, joining multiple RDDs together)\n",
    "- _RDDs_ are laid out across the cluster of machines as a collection of _partitions_, each including a subset of the data\n",
    "    - Spark then processes the objects within a partition in sequence, and processes multiple partitions in parallel\n",
    "- One simple way to create an RDD is to use ```parallelize``` method on  ```SparkContext``` with a local collection of objects\n",
    "    - first arg is the collection of objects to parallelize, in an ```Array```\n",
    "    - second arg is number of partitions to create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd = ParallelCollectionRDD[0] at parallelize at <console>:28\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at <console>:28"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(Array(1, 2, 2, 4), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- to create RDD from text file or directory of text files, pass the name of the file or directory to ```textFile``` \n",
    "    - ```textFile``` can access paths that reside on the local file system\n",
    "    - if given a directory, it will consider all of the files in that directory as part of the given RDD\n",
    "    - no data has yet been read by Spark or loaded into memory yet; instead, objects are loaded into the cluster at computation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rawblocks = linkage1 MapPartitionsRDD[4] at textFile at <console>:28\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "linkage1 MapPartitionsRDD[4] at textFile at <console>:28"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rawblocks = sc.textFile(\"linkage1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The REPL and Compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spark supports both interactive shell and compiled applications, which can be compiled and managed using _Apache Maven_\n",
    "- shell method\n",
    "    - starting work in the REPL enables quick prototyping, faster iteration, and less lag between ideas and results\n",
    "    - drawbacks: not suited for large programs, since Scala interpretation takes longer\n",
    "- hybrid method\n",
    "    - develop in the REPL, but move established pieces of code into compiled library\n",
    "    - ```spark-shell``` can use compiled JAR files with the ```--jars``` flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing Data from the Cluster to the Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RDDs have various method allowing to read data from cluster into Scala REPL\n",
    "- ```RDD.first``` returns the first element of the RDD into the client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"id_1\",\"id_2\",\"cmp_fname_c1\",\"cmp_fname_c2\",\"cmp_lname_c1\",\"cmp_lname_c2\",\"cmp_sex\",\"cmp_bd\",\"cmp_bm\",\"cmp_by\",\"cmp_plz\",\"is_match\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawblocks.first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```RDD.collect``` returns all the contents of an RDD to the client as an array\n",
    "    - not recommended for huge data sets\n",
    "- ```RDD.take``` allows us to read a given nmber of records into an array on the client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "head = Array(\"id_1\",\"id_2\",\"cmp_fname_c1\",\"cmp_fname_c2\",\"cmp_lname_c1\",\"cmp_lname_c2\",\"cmp_sex\",\"cmp_bd\",\"cmp_bm\",\"cmp_by\",\"cmp_plz\",\"is_match\", 6698,40542,1,1,1,?,1,1,1,1,1,TRUE, 45037,49220,1,?,1,?,1,1,1,1,1,TRUE, 31835,69902,1,?,1,1,1,1,1,1,1,TRUE, 4356,31352,0.875,?,1,?,1,1,1,1,1,TRUE, 45723,49837,1,?,1,?,1,1,1,1,1,TRUE, 39716,49297,1,?,1,?,1,1,1,1,1,TRUE, 71970,71971,1,?,1,?,1,1,1,1,1,TRUE, 96601,96625,1,?,1,?,1,1,1,1,1,TRUE, 28553,71491,1,?,1,?,1,1,1,1,1,TRUE)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val head = rawblocks.take(10)\n",
    "head.length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- creating an RDD does not cause distrbuted computation to take place on the cluster\n",
    "- instead, RDDs define logical data sets that are more like intermediate computation steps\n",
    "- distributed computation occurs upon invoking an _action_ on an RDD\n",
    "    - e.g. ```count``` action return # objects in RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 2, 4]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// brings objects from the RDD into local memory as an Array\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```saveAsTextFile``` saves contents of RDD to persistent storage\n",
    "    - creates a directory, and writes out each partition as a separate file\n",
    "    - this created directory might be used as an input directory by a future Spark job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.hadoop.mapred.FileAlreadyExistsException\n",
       "Message: Output directory file:/home/chtka/Projects/Spark/numbers already exists\n",
       "StackTrace:   at org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n",
       "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1184)\n",
       "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1161)\n",
       "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1161)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n",
       "  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1161)\n",
       "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1064)\n",
       "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1030)\n",
       "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1030)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n",
       "  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1030)\n",
       "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:956)\n",
       "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:956)\n",
       "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:956)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n",
       "  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:955)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1459)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1438)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1438)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n",
       "  at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1438)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.saveAsTextFile(\"numbers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1, 2, 4, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var rdd2 = sc.textFile(\"numbers\")\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```foreach``` method can be used in conjunction with ```println``` to print out each value in the array on its own line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"id_1\",\"id_2\",\"cmp_fname_c1\",\"cmp_fname_c2\",\"cmp_lname_c1\",\"cmp_lname_c2\",\"cmp_sex\",\"cmp_bd\",\"cmp_bm\",\"cmp_by\",\"cmp_plz\",\"is_match\"\n",
      "6698,40542,1,1,1,?,1,1,1,1,1,TRUE\n",
      "45037,49220,1,?,1,?,1,1,1,1,1,TRUE\n",
      "31835,69902,1,?,1,1,1,1,1,1,1,TRUE\n",
      "4356,31352,0.875,?,1,?,1,1,1,1,1,TRUE\n",
      "45723,49837,1,?,1,?,1,1,1,1,1,TRUE\n",
      "39716,49297,1,?,1,?,1,1,1,1,1,TRUE\n",
      "71970,71971,1,?,1,?,1,1,1,1,1,TRUE\n",
      "96601,96625,1,?,1,?,1,1,1,1,1,TRUE\n",
      "28553,71491,1,?,1,?,1,1,1,1,1,TRUE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- examining the data, we see a header row that we might want to remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "isHeader: (line: String)Boolean\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def isHeader(line: String) = line.contains(\"id_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6698,40542,1,1,1,?,1,1,1,1,1,TRUE, 45037,49220,1,?,1,?,1,1,1,1,1,TRUE, 31835,69902,1,?,1,1,1,1,1,1,1,TRUE, 4356,31352,0.875,?,1,?,1,1,1,1,1,TRUE, 45723,49837,1,?,1,?,1,1,1,1,1,TRUE, 39716,49297,1,?,1,?,1,1,1,1,1,TRUE, 71970,71971,1,?,1,?,1,1,1,1,1,TRUE, 96601,96625,1,?,1,?,1,1,1,1,1,TRUE, 28553,71491,1,?,1,?,1,1,1,1,1,TRUE]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head.filterNot(isHeader)\n",
    "head.filter(x => !isHeader(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shipping Code from the Client to the Cluster\n",
    "- we can interactively develop and debug data-munging code against a small amount of data that we sample from the cluster before applying to the entire data set when we're ready to transform it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "noheader = MapPartitionsRDD[6] at filter at <console>:32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[6] at filter at <console>:32"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val noheader = rawblocks.filter(x => !isHeader(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6698,40542,1,1,1,?,1,1,1,1,1,TRUE"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noheader.first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From RDDs to Data Frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spark's ```DataFrame``` is an abstraction built on top of RDDs for data sets with regular structure\n",
    "    - each row is made up of a set of columns, and each column has well-defined data type\n",
    "    - basically Spark analogue of a table in a relational databse\n",
    "    - differ from Python's ```pandas.DataFrame``` in that they represent distributed data sets on a cluster, instead of local data\n",
    "- ```SparkSession``` is a wrapper around the ```SparkContext``` object\n",
    "- can create a Data Frame from ```csv``` method on ```SparkSession```'s Reader API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prev = [_c0: string, _c1: string ... 10 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[_c0: string, _c1: string ... 10 more fields]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val prev = spark.read.csv(\"linkage1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "|  _c0|  _c1|         _c2|         _c3|         _c4|         _c5|    _c6|   _c7|   _c8|   _c9|   _c10|    _c11|\n",
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "| id_1| id_2|cmp_fname_c1|cmp_fname_c2|cmp_lname_c1|cmp_lname_c2|cmp_sex|cmp_bd|cmp_bm|cmp_by|cmp_plz|is_match|\n",
      "| 3148| 8326|           1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
      "|14055|94934|           1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
      "|33948|34740|           1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
      "|  946|71870|           1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
      "|64880|71676|           1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
      "|25739|45991|           1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
      "|62415|93584|           1|           ?|           1|           ?|      1|     1|     1|     1|      0|    TRUE|\n",
      "|27995|31399|           1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
      "| 4909|12238|           1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
      "|15161|16743|           1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
      "|31703|37310|           1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
      "|30213|36558|           1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
      "|56596|56630|           1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
      "|16481|21174|           1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
      "|32649|37094|           1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
      "|34268|37260|           1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
      "|66117|69253|           1|           ?|           1|           ?|      1|     1|     1|     1|      0|    TRUE|\n",
      "| 2771|31982|           1|           ?|           1|           ?|      0|     1|     1|     1|      1|    TRUE|\n",
      "|23557|29673|           1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prev.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spark can do some data processing while parsing, like inferring column names from a header, recognizing null values, and inferring the data types of each column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 12:===================================================>     (9 + 1) / 10]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "parsed = [id_1: int, id_2: int ... 10 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id_1: int, id_2: int ... 10 more fields]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val parsed = spark.read.\n",
    "    option(\"header\", \"true\").\n",
    "    option(\"nullValue\", \"?\").\n",
    "    option(\"inferSchema\", \"true\").\n",
    "    csv(\"linkage1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "| id_1| id_2|cmp_fname_c1|cmp_fname_c2|cmp_lname_c1|cmp_lname_c2|cmp_sex|cmp_bd|cmp_bm|cmp_by|cmp_plz|is_match|\n",
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "| 3148| 8326|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|14055|94934|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|33948|34740|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|  946|71870|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|64880|71676|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|25739|45991|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|62415|93584|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      0|    true|\n",
      "|27995|31399|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "| 4909|12238|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|15161|16743|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|31703|37310|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|30213|36558|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|56596|56630|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|16481|21174|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|32649|37094|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|34268|37260|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|66117|69253|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      0|    true|\n",
      "| 2771|31982|         1.0|        null|         1.0|        null|      0|     1|     1|     1|      1|    true|\n",
      "|23557|29673|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|37156|39557|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we can examine the schema of the ```parsed``` Data Frame with ```printSchema```\n",
    "    - each ```StructField``` contains the name of the column, the most specific data type that could handle the type of data contained in each record, a a boolean field that indicates whether a column may contain null values\n",
    "    - to do this, Spark does _two_ passes over the data set: one pass to figure out column types, and a second pass to do the actual parsing\n",
    "    - if schema is known in advance, can create instance of ```org.apache.spark.sql.types.StructType``` and pass to Reader API via ```schema``` function, possibly saving significant resources when the data set is very large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_1: integer (nullable = true)\n",
      " |-- id_2: integer (nullable = true)\n",
      " |-- cmp_fname_c1: double (nullable = true)\n",
      " |-- cmp_fname_c2: double (nullable = true)\n",
      " |-- cmp_lname_c1: double (nullable = true)\n",
      " |-- cmp_lname_c2: double (nullable = true)\n",
      " |-- cmp_sex: integer (nullable = true)\n",
      " |-- cmp_bd: integer (nullable = true)\n",
      " |-- cmp_bm: integer (nullable = true)\n",
      " |-- cmp_by: integer (nullable = true)\n",
      " |-- cmp_plz: integer (nullable = true)\n",
      " |-- is_match: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- through ```DataFrameReader``` and ```DataFrameWriter``` APIs, Spark supports reading and writing data frames in a variety of formats\n",
    "    - _json_ - similar functionality to CSV format\n",
    "    - _parquet_ and _orc_ - columnar-oriented binary file formats\n",
    "    - _jdbc_ - connects to relational database via JDBC data connection standard\n",
    "    - _libsvm_ - popular text file format for representing labeled observations with sparse features\n",
    "    - _text_ - maps each line of a file to a data frame with a single column of type ```String```\n",
    "- access ```DataFrameReader``` API through ```read``` method on a ```SparkSession``` instance\n",
    "    - load data from file using either combination of ```format``` and ```load``` methods, or one of the shortcuts for built-in formats\n",
    "- to write out data, access ```DataFrameWriter``` via ```write``` method on any DataFrame Instance\n",
    "- Spark will throw error if you try to save data frame to file that already exists by default; control this behavior using ```SaveMode``` enum, with ```Overwrite```, ```Append```, and ```Ignore``` options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// val d1 = spark.read.format(\"json\").load(\"file.json\")\n",
    "// val d2 = spark.read.json(\"file.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Data with the DataFrame API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Every time we've processed the data set, Spark has re-opened the file, reparsed the rows, and then perform the action requested\n",
    "- Instead of doing this, we can save the data in its parsed form on teh cluster\n",
    "- we can accomplish via the ```cache``` method on the Data Frame instance\n",
    "- ```cache``` call indicates that contents of DataFrame should be stored in memory the next time it's computed\n",
    "    - so in this example, the call to ```count``` does the re-opening, reparsing, and action (counting)\n",
    "    - the call to ```take``` accesses the cached data instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[id_1: int, id_2: int ... 10 more fields]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 14:==============================================>           (4 + 1) / 5]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5749132"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><td>3148</td><td>8326</td><td>1.0</td><td>NULL</td><td>1.0</td><td>NULL</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>true</td></tr>\n",
       "<tr><td>14055</td><td>94934</td><td>1.0</td><td>NULL</td><td>1.0</td><td>NULL</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>true</td></tr>\n",
       "<tr><td>33948</td><td>34740</td><td>1.0</td><td>NULL</td><td>1.0</td><td>NULL</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>true</td></tr>\n",
       "<tr><td>946</td><td>71870</td><td>1.0</td><td>NULL</td><td>1.0</td><td>NULL</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>true</td></tr>\n",
       "<tr><td>64880</td><td>71676</td><td>1.0</td><td>NULL</td><td>1.0</td><td>NULL</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>true</td></tr>\n",
       "<tr><td>25739</td><td>45991</td><td>1.0</td><td>NULL</td><td>1.0</td><td>NULL</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>true</td></tr>\n",
       "<tr><td>62415</td><td>93584</td><td>1.0</td><td>NULL</td><td>1.0</td><td>NULL</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>true</td></tr>\n",
       "<tr><td>27995</td><td>31399</td><td>1.0</td><td>NULL</td><td>1.0</td><td>NULL</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>true</td></tr>\n",
       "<tr><td>4909</td><td>12238</td><td>1.0</td><td>NULL</td><td>1.0</td><td>NULL</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>true</td></tr>\n",
       "<tr><td>15161</td><td>16743</td><td>1.0</td><td>NULL</td><td>1.0</td><td>NULL</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>true</td></tr>\n",
       "</table>"
      ],
      "text/plain": [
       "+-------+-------+-----+------+-----+------+-----+-----+-----+-----+-----+------+\n",
       "| 3148  | 8326  | 1.0 | NULL | 1.0 | NULL | 1   | 1   | 1   | 1   | 1   | true |\n",
       "| 14055 | 94934 | 1.0 | NULL | 1.0 | NULL | 1   | 1   | 1   | 1   | 1   | true |\n",
       "| 33948 | 34740 | 1.0 | NULL | 1.0 | NULL | 1   | 1   | 1   | 1   | 1   | true |\n",
       "| 946   | 71870 | 1.0 | NULL | 1.0 | NULL | 1   | 1   | 1   | 1   | 1   | true |\n",
       "| 64880 | 71676 | 1.0 | NULL | 1.0 | NULL | 1   | 1   | 1   | 1   | 1   | true |\n",
       "| 25739 | 45991 | 1.0 | NULL | 1.0 | NULL | 1   | 1   | 1   | 1   | 1   | true |\n",
       "| 62415 | 93584 | 1.0 | NULL | 1.0 | NULL | 1   | 1   | 1   | 1   | 0   | true |\n",
       "| 27995 | 31399 | 1.0 | NULL | 1.0 | NULL | 1   | 1   | 1   | 1   | 1   | true |\n",
       "| 4909  | 12238 | 1.0 | NULL | 1.0 | NULL | 1   | 1   | 1   | 1   | 1   | true |\n",
       "| 15161 | 16743 | 1.0 | NULL | 1.0 | NULL | 1   | 1   | 1   | 1   | 1   | true |\n",
       "+-------+-------+-----+------+-----+------+-----+-----+-----+-----+-----+------+"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```StorageLevel``` values indicate where data should be stored\n",
    "    - for example, ```cache()``` is shorthand for ```persist(StorageLevel.MEMORY)```, which stores rows as unserialized Java objects \n",
    "    - if a partition is estimated to not fit in memory, Spark will simply not store it and just recompute next time it's needed\n",
    "    - ```MEMORY``` level makes most sense when objects are referenced frequently or require low-latency access\n",
    "- ```MEMORY_SER``` - allocates large byte buffers in memory and serializes the records into them, taking up less space\n",
    "- ```MEMORY_AND_DISK``` and ```MEMORY_AND_DISK_SER``` will store partitions that don't fit in memory on the disk\n",
    "- both RDDs and DataFrames can cache data, but the knowledge of the data gained through a DataFrame's schema allow for far more efficient storage\n",
    "- data should be cached when likely to be referenced by multiple actions, is relatively small compared to availbale memory/disk, and is expensive to regenerate\n",
    "- RDDs are made out of ```org.apache.spark.sql.Row``` classes, which have accessor methods for getting values by index position, as well as the ```getAs[T]``` method, allowing us to look up fields of a given type by their name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 17:==============================================>           (4 + 1) / 5]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map(true -> 20931, false -> 5728201)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed.rdd.map(_.getAs[Boolean](\"is_match\")).\n",
    "    countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- problems with ```countByValue``` -> we only want to use this when we know there are only a few distinct values in the data set\n",
    "    - otherwise, we would want to use a function that won't return results to client, like ```reduceByKey```\n",
    "- if we need the results for a subsequent computation, we need to ship out the data back to the cluster by the ```parallelize``` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "letters = ParallelCollectionRDD[98] at parallelize at <console>:31\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(a,2), (c,2)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// reducebyKey example\n",
    "val letters = sc.parallelize(Array((\"a\", 1), (\"a\", 1), (\"c\", 2)))\n",
    "letters.reduceByKey((a, b) => a + b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res = [is_match: boolean, count: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[is_match: boolean, count: bigint]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val res = parsed.\n",
    "    groupBy(\"is_match\").\n",
    "    count().\n",
    "    orderBy(col(\"count\").desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+                                                              \n",
      "|is_match|  count|\n",
      "+--------+-------+\n",
      "|   false|5728201|\n",
      "|    true|  20931|\n",
      "+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame Aggregation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- other more complex aggregations likes sums, mins, maxes, means, standard deviations can be computed using ```agg``` method of DataFrame\n",
    "    - these functions are located in ```org.apache.spark.sql.functions``` package\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+                                        \n",
      "|     avg(cmp_sex)|stddev_samp(cmp_sex)|\n",
      "+-----------------+--------------------+\n",
      "|0.955001381078048|  0.2073011111689795|\n",
      "+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.avg\n",
    "import org.apache.spark.sql.functions.stddev\n",
    "\n",
    "parsed.agg(avg(col(\"cmp_sex\")), stddev(col(\"cmp_sex\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- functions are similar to components of SQL queries; we can actually treat any DataFrame as a database table and express queries using SQL syntax\n",
    "- we can create a temporary SQL table with the Spark SQL engine by the ```createOrReplaceTempView``` function from the DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed.createOrReplaceTempView(\"linkage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|is_match|    cnt|\n",
      "+--------+-------+\n",
      "|   false|5728201|\n",
      "|    true|  20931|\n",
      "+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// triple quotes are part of Scala; allow us to write multiline quotes\n",
    "spark.sql(\"\"\"\n",
    "    SELECT is_match, COUNT(*) cnt\n",
    "    FROM linkage\n",
    "    GROUP BY is_match\n",
    "    ORDER BY cnt DESC\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spark SQL vs. DataFrame API\n",
    "    - SQL is very familiar and expressive for simple queries, and is the best way to quickly read and filter data stored in columnar file formats\n",
    "    - DataFrame API shines in complex, multistage analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast Summary Statistics for DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- compute min, max, mean, and stddev of all non-null values in numerical columns of data frame by using ```describe``` (same name as in Pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 52:==============================================>           (4 + 1) / 5]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "summary = [summary: string, id_1: string ... 10 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[summary: string, id_1: string ... 10 more fields]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val summary = parsed.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- one column for each variable in the ```parsed``` DataFrame, plus an additional column named ```summary``` that indicates which metric is present in the rest of the columns of the row\n",
    "- use the ```select``` method to choose subset of columns you want to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|summary|      cmp_fname_c1|      cmp_fname_c2|\n",
      "+-------+------------------+------------------+\n",
      "|  count|           5748125|            103698|\n",
      "|   mean|0.7129024704436274|0.9000176718903216|\n",
      "| stddev|0.3887583596162788|0.2713176105782331|\n",
      "|    min|               0.0|               0.0|\n",
      "|    max|               1.0|               1.0|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary.select(\"summary\", \"cmp_fname_c1\", \"cmp_fname_c2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- to understand correlations between each feature and the value of the ```is_match``` column, we might try computing these summary statistics for the rows of the DataFrame that correspond to matches, and then for nonmatches\n",
    "- use ```where``` method with SQL-style syntax on DataFrames, or can use ```Column``` objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 54:>                                                         (0 + 4) / 5]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matches = [id_1: int, id_2: int ... 10 more fields]\n",
       "matchSummary = [summary: string, id_1: string ... 10 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[summary: string, id_1: string ... 10 more fields]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// SQL-style syntax\n",
    "val matches = parsed.where(\"is_match = true\")\n",
    "val matchSummary = matches.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-------------------+\n",
      "|summary|        cmp_fname_c1|       cmp_fname_c2|\n",
      "+-------+--------------------+-------------------+\n",
      "|  count|               20922|               1333|\n",
      "|   mean|  0.9973163859635038| 0.9898900320318174|\n",
      "| stddev|0.036506675848336785|0.08251973727615237|\n",
      "|    min|                 0.0|                0.0|\n",
      "|    max|                 1.0|                1.0|\n",
      "+-------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matchSummary.select(\"summary\", \"cmp_fname_c1\", \"cmp_fname_c2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 56:==============================================>           (4 + 1) / 5]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "misses = [id_1: int, id_2: int ... 10 more fields]\n",
       "missSummary = [summary: string, id_1: string ... 10 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[summary: string, id_1: string ... 10 more fields]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Column object syntax\n",
    "// \"filter\" and \"where\" methods are the same method\n",
    "val misses = parsed.filter(col(\"is_match\") === false)\n",
    "val missSummary = misses.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------------------+\n",
      "|summary|       cmp_fname_c1|      cmp_fname_c2|\n",
      "+-------+-------------------+------------------+\n",
      "|  count|            5727203|            102365|\n",
      "|   mean| 0.7118634802174252|0.8988473514090173|\n",
      "| stddev|0.38908060096985714|0.2727209029401023|\n",
      "|    min|                0.0|               0.0|\n",
      "|    max|                1.0|               1.0|\n",
      "+-------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "missSummary.select(\"summary\", \"cmp_fname_c1\", \"cmp_fname_c2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we want to pivot the DataFrame so that the statistics methods are the features, and the original features become the rows\n",
    "- \"wide\" form - rows of metrics and columns of variables\n",
    "- \"long\" form - rows consisting of one metric, one variable, and the value of that metric/variable pair\n",
    "- to conver from wide form to long form, can use ```flatMap``` function\n",
    "    - takes a function argument that processes each input record and returns a sequence of zero or more output records\n",
    "- need the ```schema``` object of the ```DataFrame```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- summary: string (nullable = true)\n",
      " |-- id_1: string (nullable = true)\n",
      " |-- id_2: string (nullable = true)\n",
      " |-- cmp_fname_c1: string (nullable = true)\n",
      " |-- cmp_fname_c2: string (nullable = true)\n",
      " |-- cmp_lname_c1: string (nullable = true)\n",
      " |-- cmp_lname_c2: string (nullable = true)\n",
      " |-- cmp_sex: string (nullable = true)\n",
      " |-- cmp_bd: string (nullable = true)\n",
      " |-- cmp_bm: string (nullable = true)\n",
      " |-- cmp_by: string (nullable = true)\n",
      " |-- cmp_plz: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- since every field is a string, we need to convert values from strings to doubles\n",
    "- output should be data frame w/ three columns: name of the metric, name of the column, and Double value of summary statistic for that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schema = StructType(StructField(summary,StringType,true), StructField(id_1,StringType,true), StructField(id_2,StringType,true), StructField(cmp_fname_c1,StringType,true), StructField(cmp_fname_c2,StringType,true), StructField(cmp_lname_c1,StringType,true), StructField(cmp_lname_c2,StringType,true), StructField(cmp_sex,StringType,true), StructField(cmp_bd,StringType,true), StructField(cmp_bm,StringType,true), StructField(cmp_by,StringType,true), StructField(cmp_plz,StringType,true))\n",
       "longForm = [_1: string, _2: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[_1: string, _2: string ... 1 more field]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val schema = summary.schema\n",
    "val longForm = summary.flatMap(row => {\n",
    "    val metric = row.getString(0)\n",
    "    (1 until row.size).map(i => {\n",
    "        (metric, schema(i).name, row.getString(i).toDouble)\n",
    "    })\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+-------------------+\n",
      "|   _1|          _2|                 _3|\n",
      "+-----+------------+-------------------+\n",
      "|count|        id_1|          5749132.0|\n",
      "|count|        id_2|          5749132.0|\n",
      "|count|cmp_fname_c1|          5748125.0|\n",
      "|count|cmp_fname_c2|           103698.0|\n",
      "|count|cmp_lname_c1|          5749132.0|\n",
      "|count|cmp_lname_c2|             2464.0|\n",
      "|count|     cmp_sex|          5749132.0|\n",
      "|count|      cmp_bd|          5748337.0|\n",
      "|count|      cmp_bm|          5748337.0|\n",
      "|count|      cmp_by|          5748337.0|\n",
      "|count|     cmp_plz|          5736289.0|\n",
      "| mean|        id_1|  33324.48559643438|\n",
      "| mean|        id_2|  66587.43558331935|\n",
      "| mean|cmp_fname_c1| 0.7129024704436274|\n",
      "| mean|cmp_fname_c2| 0.9000176718903216|\n",
      "| mean|cmp_lname_c1| 0.3156278193084133|\n",
      "| mean|cmp_lname_c2|0.31841283153174377|\n",
      "| mean|     cmp_sex|  0.955001381078048|\n",
      "| mean|      cmp_bd|0.22446526708507172|\n",
      "| mean|      cmp_bm|0.48885529849763504|\n",
      "+-----+------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "longForm.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```flatMap```, in general, operates on one argument and returns a sequence of 0 or values\n",
    "    - in this case, flatMap operates on a row (corresponding to each metric), and returns a sequence of tuples of the form (_metric_, _sequence_, _value_)\n",
    "- ```toDouble``` is an example of _implicit types_\n",
    "    - ```java.lang.String``` does not have a ```toDouble``` method, so Scala will try to convert String into a class that does have one\n",
    "    - in this case ```StringOps``` class has a ```toDouble``` method, and a method that can covert ```String``` to ```StringOps```\n",
    "    - so compiler converts ```String``` -> ```StringOps``` -> ```Double```\n",
    "- _implicit type conversion_ enhances functionality of core classes like ```String``` that are otherwise unmodifiable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class org.apache.spark.sql.Dataset"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longForm.getClass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```longForm``` is of type ```Dataset[T]```, which generalizes ```DataFrame``` to be able to handle more data types than just instances of the ```Row``` class\n",
    "- convert ```Dataset``` back to DataFrame through ```toDF``` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "longDF = [metric: string, field: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[metric: string, field: string ... 1 more field]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val longDF = longForm.toDF(\"metric\", \"field\", \"value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we can trasform from long form to wide form by using the ```groupBy``` operator on the column we want to use as the pivot table's row, followed by the ```pivot``` operator on the column we want to use as the pivot table's column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
